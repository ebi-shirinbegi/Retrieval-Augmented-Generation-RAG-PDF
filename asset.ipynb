{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Retrieval-Augmented Generation (RAG) System Using LLMs\n",
    "This project builds a Retrieval-Augmented Generation (RAG) system, integrating both OpenAI's GPT and Llama3.1 models. It enables document retrieval and efficient question answering, combining the power of large language models with context-based responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# Use gpt-3.5-turbo only if you have a paid api key\n",
    "#MODEL = \"gpt-3.5-turbo\"\n",
    "MODEL = \"llama3.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Model and Embeddings\n",
    "Here, based on the selected model (gpt or llama), the corresponding language model and embedding class are initialized. If OpenAI’s GPT model is used, the OpenAIEmbeddings and ChatOpenAI classes are instantiated. Otherwise, the Ollama model and embeddings are initialized. A test response is invoked to verify the model is set up correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings \n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "if MODEL.startswith(\"gpt\"):\n",
    "    model= ChatOpenAI(api_key=OPENAI_API_KEY, model=MODEL)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "else:\n",
    "    model = Ollama(model=MODEL)\n",
    "    embeddings = OllamaEmbeddings()\n",
    "    response = model.invoke(\"Where is the capital city of Afghanistan?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Output Parser and Simple Chain\n",
    "In this cell, a string output parser (StrOutputParser) is defined to parse the model's output into a string format. The chain combines the model and parser, allowing you to ask questions like \"Name 3 biggest cities of Afghanistan\" and receive parsed results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# in case gpt model is being used. \n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = model | parser\n",
    "chain.invoke(\"Name 3 biggest cities of Afghanistan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Splitting a PDF Document\n",
    "The cell uses the PyPDFLoader to load and split a PDF document into pages. This allows you to work with documents in the context of a Retrieval-Augmented Generation (RAG) system. The file afghanistan.pdf is loaded, and its content is split into manageable chunks (pages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "#You can replace your own pdf here, put your pdf file in the rag-local folder\n",
    "loader =PyPDFLoader(\"afghanistan.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Custom Question-Answer Prompt\n",
    "This cell defines a custom PromptTemplate with placeholders for context and question. The template is designed to guide the model’s response based on the provided context. The formatted prompt is tested by filling it with sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "APlease answer the question based on the below context. If you can't \n",
    "answer the question, reply \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "print(prompt.format(context=\"Here is the context\", question=\"Here is the question\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | parser\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"context\": \"The capital city of Afghanistan is Kabul\",\n",
    "        \"question\": \"Where is the capital of Afghanistan?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.input_schema.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Hugging Face Embeddings and Vector Store\n",
    "Hugging Face embeddings (sentence-transformers/all-MiniLM-L6-v2) are initialized in this cell for document vectorization. The vector store, DocArrayInMemorySearch, is created using the previously loaded PDF pages and the Hugging Face embeddings. This setup allows for efficient document retrieval in the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_documents(\n",
    "    pages, \n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving Documents Based on a Query\n",
    "In this cell, the retriever is initialized from the vector store, and a query (\"Afghanistan cities\") is invoked. The retriever searches for relevant documents from the vectorized pages based on the query, returning the most relevant results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "retriever.invoke(\"Afghanistan cities\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Document Retrieval and Question-Answering Pipeline\n",
    "In this cell, a more advanced chain is constructed to combine document retrieval with the question-answering pipeline. The retriever extracts the relevant context based on the question, and this context is passed through a series of components: the prompt template, the language model, and the output parser. This allows the model to generate a context-aware answer for the query.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    |prompt\n",
    "    |model\n",
    "    |parser\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": \"tell me about Bamiyan\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = {\n",
    "    \"What are the languages of Afghanistan?\",\n",
    "    \"How much is the population of Afghanistan\"\n",
    "}\n",
    "for question in questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {chain.invoke({'question':question})}\")\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Responses from the Model\n",
    "This cell showcases the ability to stream responses from the model. Instead of waiting for a complete answer, partial outputs are printed as they are generated by the model. This allows for real-time interaction with longer, more detailed queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in chain.stream({\"question\": \"Give me an overview of Afghanistan\"}):\n",
    "    print(s, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.batch([{\"question\": q} for q in questions])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
